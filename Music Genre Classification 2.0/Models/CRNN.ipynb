{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h63-Q2ZMaqff",
        "outputId": "d05c551e-d68b-467a-e651-00725248c2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3NF9UzpbUe8",
        "outputId": "02347e93-7b5d-4729-e068-5f0821d61b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: python_speech_features\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5867 sha256=21f1b50174a068da8c821cebdfc27aab3c74b138ed036df7e8c49138aabff349\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
            "Successfully built python_speech_features\n",
            "Installing collected packages: python_speech_features\n",
            "Successfully installed python_speech_features-0.6\n"
          ]
        }
      ],
      "source": [
        "pip install python_speech_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UChzvpbIbbmR",
        "outputId": "75a70500-e284-404c-b52f-041146159c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 2.2719\n",
            "Epoch [2/50], Loss: 2.0430\n",
            "Epoch [3/50], Loss: 1.7489\n",
            "Epoch [4/50], Loss: 1.5983\n",
            "Epoch [5/50], Loss: 1.4840\n",
            "Epoch [6/50], Loss: 1.4081\n",
            "Epoch [7/50], Loss: 1.4039\n",
            "Epoch [8/50], Loss: 1.3462\n",
            "Epoch [9/50], Loss: 1.2623\n",
            "Epoch [10/50], Loss: 1.2477\n",
            "Epoch [11/50], Loss: 1.2159\n",
            "Epoch [12/50], Loss: 1.2023\n",
            "Epoch [13/50], Loss: 1.1978\n",
            "Epoch [14/50], Loss: 1.1491\n",
            "Epoch [15/50], Loss: 1.1196\n",
            "Epoch [16/50], Loss: 1.1578\n",
            "Epoch [17/50], Loss: 1.0827\n",
            "Epoch [18/50], Loss: 1.0502\n",
            "Epoch [19/50], Loss: 1.0371\n",
            "Epoch [20/50], Loss: 1.0259\n",
            "Epoch [21/50], Loss: 0.9903\n",
            "Epoch [22/50], Loss: 0.9470\n",
            "Epoch [23/50], Loss: 0.9843\n",
            "Epoch [24/50], Loss: 0.9525\n",
            "Epoch [25/50], Loss: 0.9522\n",
            "Epoch [26/50], Loss: 0.9239\n",
            "Epoch [27/50], Loss: 0.9303\n",
            "Epoch [28/50], Loss: 0.8749\n",
            "Epoch [29/50], Loss: 0.8503\n",
            "Epoch [30/50], Loss: 0.9045\n",
            "Epoch [31/50], Loss: 0.9155\n",
            "Epoch [32/50], Loss: 0.8782\n",
            "Epoch [33/50], Loss: 0.8124\n",
            "Epoch [34/50], Loss: 0.8192\n",
            "Epoch [35/50], Loss: 0.8059\n",
            "Epoch [36/50], Loss: 0.7781\n",
            "Epoch [37/50], Loss: 0.7701\n",
            "Epoch [38/50], Loss: 0.6662\n",
            "Epoch [39/50], Loss: 0.6714\n",
            "Epoch [40/50], Loss: 0.7246\n",
            "Epoch [41/50], Loss: 0.7492\n",
            "Epoch [42/50], Loss: 0.6949\n",
            "Epoch [43/50], Loss: 0.6417\n",
            "Epoch [44/50], Loss: 0.6729\n",
            "Epoch [45/50], Loss: 0.6711\n",
            "Epoch [46/50], Loss: 0.6322\n",
            "Epoch [47/50], Loss: 0.6064\n",
            "Epoch [48/50], Loss: 0.5597\n",
            "Epoch [49/50], Loss: 0.5519\n",
            "Epoch [50/50], Loss: 0.5359\n",
            "Model saved to /content/drive/MyDrive/Deep Learning/crnn_model.pth\n",
            "Accuracy: 0.593939393939394\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from python_speech_features import mfcc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the CRNN model\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, input_size, num_filters, rnn_hidden_size, output_size):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters*2, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.rnn = nn.LSTM(input_size=num_filters*2, hidden_size=rnn_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size*2, output_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch, seq_len, features)\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch, seq_len, input_size)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = x[:, -1, :]  # Take the output of the last time step\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Load the audio dataset\n",
        "def loadDataset(directory, max_folders):\n",
        "    dataset = []\n",
        "    i = 0\n",
        "    for folder in os.listdir(directory):\n",
        "        i += 1\n",
        "        if i > max_folders:\n",
        "            break\n",
        "        for file in os.listdir(os.path.join(directory, folder)):\n",
        "            file_path = os.path.join(directory, folder, file)\n",
        "            if file == \".DS_Store\" or os.path.isdir(file_path):\n",
        "                continue\n",
        "            try:\n",
        "                (rate, sig) = wav.read(file_path)\n",
        "                mfcc_feat = mfcc(sig, rate, winlen=0.020, appendEnergy=False)\n",
        "                feature = (mfcc_feat, folder)\n",
        "                dataset.append(feature)\n",
        "            except ValueError as e:\n",
        "                print(f\"Skipping file '{file_path}': {str(e)}\")\n",
        "    return dataset\n",
        "\n",
        "# Load the dataset\n",
        "directory = \"/content/drive/MyDrive/Deep Learning/genres_original\"\n",
        "dataset = loadDataset(directory, max_folders=100)\n",
        "\n",
        "# Find the maximum length of the features\n",
        "max_length = max(len(data[0]) for data in dataset)\n",
        "\n",
        "# Pad or truncate sequences to the maximum length\n",
        "def pad_or_truncate(feature, max_length):\n",
        "    length = feature.shape[0]\n",
        "    if length > max_length:\n",
        "        return feature[:max_length]\n",
        "    elif length < max_length:\n",
        "        padded_feature = np.zeros((max_length, feature.shape[1]))\n",
        "        padded_feature[:length] = feature\n",
        "        return padded_feature\n",
        "    return feature\n",
        "\n",
        "# Process features\n",
        "X = np.array([pad_or_truncate(data[0], max_length) for data in dataset])\n",
        "y = np.array([data[1] for data in dataset])\n",
        "\n",
        "# Normalize the input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = np.array([scaler.fit_transform(x) for x in X])\n",
        "\n",
        "# Convert labels to integers using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
        "y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(device)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.33, random_state=1)\n",
        "\n",
        "# Create DataLoader for batch processing\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the neural network hyperparameters\n",
        "input_size = X_scaled[0].shape[1]  # Number of features (13 for MFCC)\n",
        "num_filters = 32  # Reduced for faster training\n",
        "rnn_hidden_size = 64  # Reduced for faster training\n",
        "output_size = len(np.unique(y_encoded))\n",
        "\n",
        "# Create the CRNN model\n",
        "model = CRNN(input_size, num_filters, rnn_hidden_size, output_size).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the CRNN model\n",
        "num_epochs = 50  # Reduced for faster experimentation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Save the model after training\n",
        "model_save_path = \"/content/drive/MyDrive/Deep Learning/crnn_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# Evaluate the CRNN model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvqI1thSA368",
        "outputId": "9989a0fc-c98c-4281-bfd7-84e5734df5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted label: blues\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-4658ffa4bc34>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_load_path))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from python_speech_features import mfcc\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the CRNN model (same as used during training)\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, input_size, num_filters, rnn_hidden_size, output_size):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters*2, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.rnn = nn.LSTM(input_size=num_filters*2, hidden_size=rnn_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size*2, output_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch, seq_len, features)\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch, seq_len, input_size)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = x[:, -1, :]  # Take the output of the last time step\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Load the CRNN model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = 13  # Number of MFCC features\n",
        "num_filters = 32\n",
        "rnn_hidden_size = 64\n",
        "output_size = len(label_encoder.classes_)  # Number of classes\n",
        "model = CRNN(input_size, num_filters, rnn_hidden_size, output_size).to(device)\n",
        "\n",
        "model_load_path = \"/content/drive/MyDrive/Deep Learning/crnn_model.pth\"\n",
        "model.load_state_dict(torch.load(model_load_path))\n",
        "model.eval()\n",
        "\n",
        "# Define preprocessing and transformation functions\n",
        "def preprocess_audio(file_path, max_length=1000):\n",
        "    rate, sig = wav.read(file_path)\n",
        "    mfcc_feat = mfcc(sig, rate, winlen=0.020, appendEnergy=False)\n",
        "    padded_mfcc = pad_or_truncate(mfcc_feat, max_length)\n",
        "    return padded_mfcc\n",
        "\n",
        "def pad_or_truncate(feature, max_length):\n",
        "    length = feature.shape[0]\n",
        "    if length > max_length:\n",
        "        return feature[:max_length]\n",
        "    elif length < max_length:\n",
        "        padded_feature = np.zeros((max_length, feature.shape[1]))\n",
        "        padded_feature[:length] = feature\n",
        "        return padded_feature\n",
        "    return feature\n",
        "\n",
        "# Load and preprocess the audio file\n",
        "audio_path = \"/content/drive/MyDrive/Deep Learning/genres_original/pop/pop.00005.wav\"\n",
        "mfcc_features = preprocess_audio(audio_path)\n",
        "mfcc_features = StandardScaler().fit_transform(mfcc_features)  # Normalize\n",
        "mfcc_features_tensor = torch.tensor(mfcc_features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():\n",
        "    output = model(mfcc_features_tensor)\n",
        "    _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "# Map prediction to label\n",
        "predicted_label = label_encoder.inverse_transform([predicted_class.item()])[0]\n",
        "print(f\"Predicted label: {predicted_label}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}